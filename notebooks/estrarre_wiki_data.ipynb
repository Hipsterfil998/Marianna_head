{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPTYHXRGUnlvIfoN0VzbN/2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IWE0Tt13kGPP"},"outputs":[],"source":["import pandas as pd\n","import wikipedia\n","import urllib.parse\n","\n","\n","url = 'https://docs.google.com/spreadsheets/d/1_nMf9q8Hx3t8Q3AnEsqrMkRoPZNnhW57BveGQanCXT0/export?format=csv'\n","df = pd.read_csv(url)\n","#print(df)\n","\n","\n","urls = df['link'].to_list()\n","\n","def get_last_segment(url):\n","    # Usa il metodo rsplit per suddividere l'URL dall'ultimo '/'\n","    segments = url.rsplit('/', 1)\n","    # Estrai l'ultimo segmento\n","    last_segment = segments[-1] if segments else ''\n","    # Decodifica l'URL percent-encoded\n","    last_segment = urllib.parse.unquote(last_segment)\n","    # Rimuovi le parentesi tonde\n","    last_segment = last_segment.replace('(', '').replace(')', '')\n","    return last_segment\n","\n","\n","\n","\n","def segmenta_contenuto(content):\n","    capitoletti = {}\n","    lines = content.split('\\n')\n","    current_capitolo = ''\n","    current_sottocapitolo = ''\n","    current_testo = ''\n","\n","    for line in lines:\n","        # Gestisce i capitoli principali (==)\n","        if line.startswith('== ') and line.endswith(' =='):\n","            # Salva il contenuto precedente se esiste\n","            if current_sottocapitolo:\n","                if current_capitolo not in capitoletti:\n","                    capitoletti[current_capitolo] = {}\n","                capitoletti[current_capitolo][current_sottocapitolo] = current_testo.strip()\n","                current_sottocapitolo = ''\n","            elif current_capitolo:\n","                if isinstance(capitoletti.get(current_capitolo), str):\n","                    # Se è già una stringa, mantienila\n","                    pass\n","                else:\n","                    capitoletti[current_capitolo] = current_testo.strip()\n","\n","            # Imposta il nuovo capitolo\n","            current_capitolo = line[3:-3].strip()\n","            current_testo = ''\n","            capitoletti[current_capitolo] = {}\n","\n","        # Gestisce i sottocapitoli (===)\n","        elif line.startswith('=== ') and line.endswith(' ==='):\n","            # Salva il contenuto precedente del sottocapitolo se esiste\n","            if current_sottocapitolo:\n","                if current_capitolo not in capitoletti:\n","                    capitoletti[current_capitolo] = {}\n","                capitoletti[current_capitolo][current_sottocapitolo] = current_testo.strip()\n","\n","            # Imposta il nuovo sottocapitolo\n","            current_sottocapitolo = line[4:-4].strip()\n","            current_testo = ''\n","\n","        # Aggiunge il testo alla sezione corrente\n","        else:\n","            current_testo += line + '\\n'\n","\n","    # Salva l'ultimo contenuto\n","    if current_sottocapitolo:\n","        if current_capitolo not in capitoletti:\n","            capitoletti[current_capitolo] = {}\n","        capitoletti[current_capitolo][current_sottocapitolo] = current_testo.strip()\n","    elif current_capitolo:\n","        if isinstance(capitoletti.get(current_capitolo), dict) and not capitoletti[current_capitolo]:\n","            capitoletti[current_capitolo] = current_testo.strip()\n","        elif not isinstance(capitoletti.get(current_capitolo), dict):\n","            capitoletti[current_capitolo] = current_testo.strip()\n","\n","    return capitoletti\n","\n","\n","def get_wikipedia_page_details(page_title):\n","\tpage = wikipedia.page(page_title)\n","\tif page:\n","\t\tcontent_chapters = segmenta_contenuto(page.content)\n","\t\treturn {\n","\t\t\t'summary': page.summary,\n","\t\t\t'title': page.title,\n","\t\t\t'url': page.url,\n","\t\t\t'content': content_chapters,#page.content,  # Contenuto completo\n","\t\t\t'links': list(page.links)  # Tutti i link\n","\t\t}\n","\telse:\n","\t\treturn {\n","\t\t\t'summary': page_summary,\n","\t\t\t'title': page_title,\n","\t\t\t'url': None,\n","\t\t\t'content': None,\n","\t\t\t'links': None\n","\t\t}\n","\n","\n","\n","\n","\n","wikipedia.set_lang(\"it\")\n","data = []\n","for x in urls:\n","\ta = get_last_segment(x)\n","\tpage_details = get_wikipedia_page_details(a)\n","\n","\tdata.append(page_details)\n","\n","\n","new = pd.DataFrame(data)\n","new.to_csv(\"/run/media/raffaele/volumni/sichelia/capa_marianna/extracted_data_wiki/wiki_naples.tsv\", sep='\\t', index=False)\n","#print(new['content'][0])\n","#abba = new['content'][0]\n","#babba = new['content'][1]\n","\n","#print(abba.keys())\n","#print(babba.keys())\n","#print(new['links'][0])\n","\n","\n","#capitoletti = segmenta_contenuto(new['content'])\n","#print(capitoletti)"]}]}